{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install PyPDF2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install transformers==4.12.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install pytesseract\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import PyPDF2\n",
    "from PIL import Image\n",
    "import pytesseract\n",
    "\n",
    "# Directory for storing PDF resumes and job applications\n",
    "pdf_directory = 'C:/Users/amrit/Downloads/datacamp/LLM AI - Datacamp/pdf_files'\n",
    "#C:\\Users\\amrit\\Downloads\\datacamp\\LLM AI - Datacamp\n",
    "\n",
    "# Directory for storing extracted text from PDFs\n",
    "text_directory = 'C:/Users/amrit/Downloads/datacamp/LLM AI - Datacamp/extracted_text'\n",
    "\n",
    "# OCR output directory for scanned PDFs\n",
    "ocr_directory = 'C:/Users/amrit/Downloads/datacamp/LLM AI - Datacamp/ocr_output'\n",
    "\n",
    "# Create directories if they don't exist\n",
    "os.makedirs(pdf_directory, exist_ok=True)\n",
    "os.makedirs(text_directory, exist_ok=True)\n",
    "os.makedirs(ocr_directory, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "for file_name in os.listdir(pdf_directory):\n",
    "    if file_name.endswith('.pdf'):\n",
    "        # Open the PDF file\n",
    "        with open(os.path.join(pdf_directory, file_name), 'rb') as file:\n",
    "            # Create a PDF reader object\n",
    "            reader = PyPDF2.PdfReader(file)\n",
    "\n",
    "            # Extract text from each page\n",
    "            text = ''\n",
    "            for page in reader.pages:\n",
    "                text += page.extract_text()\n",
    "\n",
    "            # Save the extracted text as a text file\n",
    "            text_file_name = file_name.replace('.pdf', '.txt')\n",
    "            text_file_path = os.path.join(text_directory, text_file_name)\n",
    "            with open(text_file_path, 'w') as text_file:\n",
    "                text_file.write(text)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Optional Step\n",
    "for file_name in os.listdir(pdf_directory):\n",
    "    if file_name.endswith('.pdf'):\n",
    "        # Open the PDF file\n",
    "        with Image.open(os.path.join(pdf_directory, file_name)) as img:\n",
    "            # Perform OCR using pytesseract\n",
    "            ocr_text = pytesseract.image_to_string(img, lang='eng')\n",
    "\n",
    "            # Save the OCR output as a text file\n",
    "            ocr_file_name = file_name.replace('.pdf', '.txt')\n",
    "            ocr_file_path = os.path.join(ocr_directory, ocr_file_name)\n",
    "            with open(ocr_file_path, 'w') as ocr_file:\n",
    "                ocr_file.write(ocr_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chapter Summary 1:\n",
      "<pad> amrita Kanchan is a Computer science engineering graduate from the. SHRI RAMSWAROOP MEMORIAL COLLEGE OF ENGINEERING AND MANAGEMENT, Lucknow. some of my technical skills are MySQL, Python, Excel, Statistics, GCP, Tableau, Power BI, AWS, C#, ASP.NET, Java, JavaScript, HTML,CSS, Django, C. my hobbies include reading, net surfing and learning new skills.</s>\n",
      "\n",
      "Chapter Summary 2:\n",
      "<pad> AI-powered recommendations Netflix blog, Medium1 LARGE LANGUAGE MODELS (LLMS) CONCEPTS AI and data-driven tasks Sentiment analysis, fraud detection, and more Still, lacked human-like interaction Enter Large Language Models Unsplash1 LARGE LANGUAGE MODELS (LLMS) CONCEPTS Business opportunities Benefits Automate tasks Improve efficiency Create revenue streams Enable new capabilities The possibilities are endless!</s>\n",
      "\n",
      "Chapter Summary 3:\n",
      "<pad> \"to Kill a Mockingbird\" is my all-time favorite book. \"sarcasm is a powerful story about prejudice, justice, and the human experience,\" says author. \"stokenization\" splits text into individual words, or tokens. \"stokenization\" converts text into a list of words, or tokens,. \"stokenization\" converts text into a list of words.</s>\n",
      "\n",
      "Chapter Summary 4:\n",
      "<pad> LARGE LANGUAGE MODELS (LLMS) is a generative language modeling tool. it is based on learnings from generative pre-training. a transformer is a language model that processes multiple parts at the same time. it can be used to predict the next word and generate coherent text. a transformer can be used to predict the next word based on learnings from training data, for example: \"the quick brown fox jumps over the lazy dog\"</s>\n",
      "\n",
      "Chapter Summary 5:\n",
      "<pad> data concerns and considerations LARGE LANGUAGE MODELS (LLMS) CONCEPTS Data quality Quality data is essential Accurate data = better learning = improved response quality = increased trust A child learning to talk Gibberish-in -> gibberish-out LARGE LANGUAGE MODELS (LLMS) CONCEPTS Ethical and environmental concerns LARGE LANGUAGE MODELS (LLMS) CONCEPTS Information hazards Disseminating harmful information Disseminating</s>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Directory for storing PDF resumes and job applications\n",
    "pdf_directory = 'C:/Users/amrit/Downloads/datacamp/LLM AI - Datacamp/pdf_files'\n",
    "\n",
    "resume_files = []\n",
    "for file_name in os.listdir(pdf_directory):\n",
    "    if file_name.endswith('.pdf'):\n",
    "        resume_files.append(os.path.join(pdf_directory, file_name))\n",
    "\n",
    "resume_summaries = []  # To store the generated summaries\n",
    "\n",
    "# Loop through each resume file\n",
    "for resume_file in resume_files:\n",
    "    with open(resume_file, 'rb') as file:\n",
    "        # Create a PDF reader object\n",
    "        reader = PyPDF2.PdfReader(file)\n",
    "\n",
    "        # Extract text from each page\n",
    "        text = ''\n",
    "        for page in reader.pages:\n",
    "            text += page.extract_text()\n",
    "            \n",
    "# Continuing the loop from the previous step\n",
    "        from transformers import T5ForConditionalGeneration,T5Tokenizer\n",
    "\n",
    "        # Initialize the model and tokenizer\n",
    "        model = T5ForConditionalGeneration.from_pretrained(\"t5-base\")\n",
    "        tokenizer = T5Tokenizer.from_pretrained(\"t5-base\")\n",
    "\n",
    "        # Encode the text\n",
    "        inputs = tokenizer.encode(\"summarize: \" + text, \n",
    "        return_tensors=\"pt\", max_length=1000, \n",
    "        truncation=True)\n",
    "\n",
    "        # Generate the summary\n",
    "        outputs = model.generate(inputs, \n",
    "        max_length=1000, min_length=100, \n",
    "        length_penalty=2.0, num_beams=4, \n",
    "        early_stopping=True)\n",
    "\n",
    "        # Decode the summary\n",
    "        summary = tokenizer.decode(outputs[0])\n",
    "\n",
    "        resume_summaries.append(summary)\n",
    "\n",
    "# Print the generated summaries for each resume\n",
    "for i, summary in enumerate(resume_summaries):\n",
    "    print(f\"Chapter Summary {i+1}:\")\n",
    "    print(summary)\n",
    "    print()\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Data concerns and\\nconsiderations\\nLARGE LANGUAGE MODELS (LLMS) CONCEPTS\\nVidhi Chugh\\nAI strategist and ethicist\\nLARGE LANGUAGE MODELS (LLMS) CONCEPTS\\nData considerations\\n \\n  \\nData volume and compute power\\nData quality\\nLabeling\\nBias\\nPrivacy\\nLARGE LANGUAGE MODELS (LLMS) CONCEPTS\\nData volume and compute power\\nLLMs need a lot of data\\nSimilar to a child learning to talk\\n570 GB, ~1.3 million books \\n Freepik1\\nLARGE LANGUAGE MODELS (LLMS) CONCEPTSData volume and compute power\\nLLMs need a lot of data\\nSimilar to a child learning to talk\\n570 GB, ~1.3 million books\\n \\nExtensive computing power; think of the\\nenergy consumption\\n \\nCan cost millions of dollars!\\nLARGE LANGUAGE MODELS (LLMS) CONCEPTS\\nData quality\\nQuality data is essential\\n \\nAccurate data = better learning = improved\\nresponse quality = increased trust\\n \\nA child learning to talk\\nGibberish-in -> gibberish-out\\nLARGE LANGUAGE MODELS (LLMS) CONCEPTS\\nLabeled data\\nCorrect data label: accurate learning, generalize patterns, accurate responses\\nLabor-intensive: assigning correct label to each article\\nIncorrect labels impact model performance\\nAddress errors: identify -> analyze -> iterate\\nLARGE LANGUAGE MODELS (LLMS) CONCEPTS\\nData bias\\nInfluenced by societal stereotypes\\nLack of diversity in training data\\nDiscrimination and unfair outcomes\\n \\nSpot and deal with the biased data\\nEvaluate data imbalances\\nPromote diversity\\nBias mitigation techniques: more diverse\\nexamples\\nExample:\\n\"The nurse said that...\" -> \"she\" or \"her\"\\nLARGE LANGUAGE MODELS (LLMS) CONCEPTS\\nData privacy\\nCompliance with data protection and\\nprivacy regulations\\n \\nPrivacy is a concern\\nTraining on data without permission can\\nlead to a breach\\nLegal, financial and reputational harmSensitive or personally identifiable\\ninformation (PII)\\n \\nGet permission\\nLet\\'s practice!\\nLARGE LANGUAGE MODELS (LLMS) CONCEPTS\\nEthical and\\nenvironmental\\nconcerns\\nLARGE LANGUAGE MODELS (LLMS) CONCEPTS\\nVidhi Chugh\\nAI strategist and ethicist\\nLARGE LANGUAGE MODELS (LLMS) CONCEPTS\\nEthical concerns\\nTransparency risk\\nLARGE LANGUAGE MODELS (LLMS) CONCEPTSEthical concerns\\nTransparency risk\\nAccountability risk -\\nLARGE LANGUAGE MODELS (LLMS) CONCEPTSEthical concerns\\nTransparency risk\\nAccountability risk\\nInformation hazards\\nLARGE LANGUAGE MODELS (LLMS) CONCEPTS\\nTransparency risk\\nChallenging to understand the output\\n \\nDifficult to identify issues\\nBias\\nErrors\\nMisuse\\nBlack box\\n \\nExample: reasoning behind predicting\\ndisease outcomes\\nLARGE LANGUAGE MODELS (LLMS) CONCEPTS\\nAccountability risk\\nResponsibility of LLMs\\' actions\\n \\nWho is responsible?\\nIncorrect and harmful advice\\nModel developer or the company?\\n \\nGame without rules\\nNo transparency\\nNo accountability \\n Freepik1\\nLARGE LANGUAGE MODELS (LLMS) CONCEPTS\\nInformation hazards\\nDisseminating harmful information\\nHarmful content generation\\nMisinformation spread\\nMalicious use\\nToxicity\\nLARGE LANGUAGE MODELS (LLMS) CONCEPTS\\nInformation hazards\\n \\nHarmful content generation\\nHarmful, offensive, or inappropriate\\n \\nPrompt or biased training data\\n \\nExample:\\nBullying vs. friendly school environment\\nDistressing and harmful \\nMisinformation spread\\nGenerate text on any topic\\n \\nBut, no verification!\\n \\nExample:\\n\"What\\'s a good diet for losing weight?\"\\nUnsubstantiated diet plan\\nLARGE LANGUAGE MODELS (LLMS) CONCEPTS\\nInformation hazards\\n \\nMalicious use\\nBad actors exploiting LLMs\\n \\nGenerate deceptive content\\n \\nExample:\\nFabricated news\\nManipulating public and causing unrest \\nToxicity\\nInappropriate content\\n \\nTraining or through manipulated prompts\\n \\nExample:\\nInsensitive response\\nStereotype\\nLARGE LANGUAGE MODELS (LLMS) CONCEPTS\\nEnvironmental concerns\\n \\nEcological footprint of LLMs\\n \\nSubstantial energy resources to train\\n \\nImpact through carbon emissions\\n Freepik1\\nLARGE LANGUAGE MODELS (LLMS) CONCEPTS\\nCooling requires electricity too!\\nProduce considerable heat that needs\\ncooling\\n \\nImagine thousands of laptops overheating\\nRequire complex cooling systems\\nAdds to environmental impact\\n \\nBalance the cost and benefits\\nUse renewable energy\\nEnergy-efficient tech \\n Freepik1\\nLet\\'s practice!\\nLARGE LANGUAGE MODELS (LLMS) CONCEPTS\\nWhere are LLMs\\nheading?\\nLARGE LANGUAGE MODELS (LLMS) CONCEPTS\\nVidhi Chugh\\nAI strategist and ethicist\\nLARGE LANGUAGE MODELS (LLMS) CONCEPTSJourney so far\\nLARGE LANGUAGE MODELS (LLMS) CONCEPTSJourney so far\\nLARGE LANGUAGE MODELS (LLMS) CONCEPTSJourney so far\\nLARGE LANGUAGE MODELS (LLMS) CONCEPTSJourney so far\\nLARGE LANGUAGE MODELS (LLMS) CONCEPTS\\nModel explainability\\n \\nHow do they arrive at their outputs?\\n \\nRoad-trip planning\\nWhy this particular route?\\nWhy these specific spots?\\n \\nBuilds trust and transparency\\nIdentify and correct the biases or errors \\n Freepik1\\nLARGE LANGUAGE MODELS (LLMS) CONCEPTS\\nEfficiency\\nComputational efficiency\\nHigh-quality output with less compute\\nFaster and efficient\\nModel compression\\nOptimization\\nBenefits: better storage, lower energy use\\nAccessibility and sustainability\\nPromotes green AI\\nReduces operating costs\\n Freepik1\\nLARGE LANGUAGE MODELS (LLMS) CONCEPTS\\nUnsupervised bias handling\\nBiased data -> discrimination\\nUnsupervised bias handling\\nBias detection and mitigation techniques,\\nautomatically\\nNo need of explicit human-labeled data\\nIdentifies and reduces by analyzing\\npatterns\\nChallenge\\nSubtle, difficult to detect\\nMight introduce new biases\\nLARGE LANGUAGE MODELS (LLMS) CONCEPTS\\nEnhanced creativity\\nCreativity in text-based and visual art\\nforms\\nArtistic content: learned patterns, not\\nemotional understanding\\nLack human-like comprehension of art or\\nemotions\\nDemonstrate human-like emotional\\nbehavior\\nFuture: emotion inference\\n https://arxiv.org/pdf/2302.09582.pdf1\\nLet\\'s practice!\\nLARGE LANGUAGE MODELS (LLMS) CONCEPTS\\nTime to wrap-up\\nLARGE LANGUAGE MODELS (LLMS) CONCEPTS\\nVidhi Chugh\\nAI strategist and ethicist\\nLARGE LANGUAGE MODELS (LLMS) CONCEPTS\\nHow far we have come!\\nLLMs transforming interaction with technology\\nLARGE LANGUAGE MODELS (LLMS) CONCEPTS\\nHow far we have come!\\n \\nSubstantial data requirements\\n \\nChallenges and risks - privacy, ethics, and environmental implications\\n \\nFuture research and development\\nLARGE LANGUAGE MODELS (LLMS) CONCEPTS\\nThere is more to it\\n \\nEntire teams devoted to understanding LLMs\\n \\nExciting times ahead\\n \\nStay updated with the latest developments\\nMore on data ethics\\nIntroduction to ChatGPT\\nCongratulations!\\nLARGE LANGUAGE MODELS (LLMS) CONCEPTS\\n'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
